{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset and question\n",
    "---\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "    This dataset contains information about 21 features of 146 people of Enron which went bankrupt for huge fraud scandal. There were some people of interests(POI) who actively participated in this fraud scandal and gained unjusted profit. This report aims to find certain criteria that can distinguish POI effectively. With this criteria, we can find some suspicious people who were classified as non-POI or identify new person whether he is POI or not. \n",
    "\n",
    "## 1) Load datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  scipy.stats as stats\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "###\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi', 'def_income_r', 'es_r', 'from_r', 'to_r']  # You will need to use more features  # You will need to use more features\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "my_dataset = data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Remove outliers\n",
    "---\n",
    "    I found one sample extremely larger than the others, which was total. I decided to delete from my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "del my_dataset['TOTAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Features\n",
    "---\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "    There were three categories for features.\n",
    "    \n",
    "    1) financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "\n",
    "    2) email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "\n",
    "    3) POI label: [‘poi’] (boolean, represented as integer)\n",
    "    \n",
    "    \n",
    "\n",
    "## 1) Select features\n",
    "---\n",
    "    The overall strategy for selecting features was that include as many features as possible except for the too irrelevent features that would only cause noise. At first, I excluded the email_address which definatly would not help. Next, I picked some variables based on my hypothesis\n",
    "    \n",
    "    1) POI could take advantage of company(Amount matter):'salary', 'bonus', 'long_term_incentive', 'deferral_payments', 'loan_advances', 'other', 'total_stock_value'\n",
    "    \n",
    "    2) POI could contact with the other POI closely: 'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi'\n",
    "\n",
    "## 2) Create new features\n",
    "---\n",
    "    I thought of several more hypothesis to add new features.\n",
    "    \n",
    "    1) POI could have lower deferred ratio because they had inside information(Ratio matter): \n",
    "    def_income_r = ('deferred_income'/'salary'+'bonus'+'long_term_incentive')\n",
    "    es_r = ('exercised_stock_options'/'total_stock_value')\n",
    "    \n",
    "    2) Interaction could be captured better in ratio than amount of email.\n",
    "    from_r = ('from_poi_to_this_person'/'from_messages')\n",
    "    to_r = ('from_this_person_to_poi'/'to_messages')\n",
    "    \n",
    "    As a result, the 13 features I chose are following.\n",
    "    - 'poi','salary', 'bonus', 'long_term_incentive', 'deferral_payments', 'loan_advances', 'other', 'total_stock_value', 'shared_receipt_with_poi', 'def_income_r', 'es_r', 'from_r', 'to_r'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Create new feature(s)\n",
    "for i in my_dataset:\n",
    "    try:\n",
    "        my_dataset[i]['def_income_r'] = my_dataset[i]['deferred_income'] / -float((my_dataset[i]['salary'] + my_dataset[i]['bonus'] + my_dataset[i]['long_term_incentive']))\n",
    "    except:\n",
    "        my_dataset[i]['def_income_r'] = 'NaN'\n",
    "    try:\n",
    "        my_dataset[i]['es_r'] = abs(my_dataset[i]['exercised_stock_options'] / float(my_dataset[i]['total_stock_value']))\n",
    "    except:\n",
    "        my_dataset[i]['es_r'] = 'NaN'\n",
    "    try:\n",
    "        my_dataset[i]['from_r'] = my_dataset[i]['from_poi_to_this_person'] / float(my_dataset[i]['from_messages'])\n",
    "    except:\n",
    "        my_dataset[i]['from_r'] = 'NaN'\n",
    "    try:\n",
    "        my_dataset[i]['to_r'] = my_dataset[i]['from_this_person_to_poi'] / float(my_dataset[i]['to_messages'])\n",
    "    except:\n",
    "        my_dataset[i]['to_r'] = 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feature Labels Split\n",
    "---\n",
    "    I splited the label(POI) from the rest of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "features_list = ['poi','salary', 'bonus', 'long_term_incentive', 'deferral_payments', 'loan_advances', 'other', 'total_stock_value',\\\n",
    "                'shared_receipt_with_poi', 'def_income_r', 'es_r', 'from_r', 'to_r']\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Process 'Nan'\n",
    "---\n",
    "    This part was tricky, but very important since I put a lot of features in the model and there are lots of 'NaN' in dataset. I decided to put mean value because I'll later scale all of the features with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "from sklearn.preprocessing import Imputer\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0).fit(features)\n",
    "features_im = imr.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature scaling\n",
    "---\n",
    "    Since all the features have different scale, I standardize with each of mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_sc = scaler.fit_transform(features_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) PCA\n",
    "---\n",
    "    I thought 12 features are still too much for 146 observations. I tried to capture the at least 90% of variance of features. As a result, 8 principle components were chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342054552748\n",
      "0.468594267683\n",
      "0.585045071051\n",
      "0.67941776334\n",
      "0.753380762723\n",
      "0.818048080509\n",
      "0.871223661843\n",
      "0.915566013996\n",
      "0.947583702188\n",
      "0.970328918321\n",
      "0.990868838914\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAFkCAYAAABvkjJwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d3a9610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "explained =[]\n",
    "\n",
    "for i in range(1,len(features_sc[0]+1)):\n",
    "    pca = PCA(n_components = i)\n",
    "    pca.fit(features_sc)\n",
    "    explained_ratio = pca.explained_variance_ratio_\n",
    "    explained.append(sum(explained_ratio))\n",
    "    print sum(explained_ratio)\n",
    "\n",
    "plt.plot(explained)\n",
    "plt.ylabel('explained_ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algorithms\n",
    "---\n",
    "WhatWhat algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”] algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Metric for algorithms\n",
    "---\n",
    "    There are two labels for each observation. However, the amount of both labels differed greatly. So I decided to use precision and accuracy to evaluate the models. Precision was especially important because there are already too few POI so that just assuming all the people non-POI would result high accuracy. First, I need to see the portion of POI among the whole. 13.4 percent of the people were POI so that precision should be better than that result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.134328358209\n"
     ]
    }
   ],
   "source": [
    "standard = sum(labels)/len(labels)\n",
    "print standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Algorithms candidates\n",
    "---\n",
    "    I've selected 5 candidates for algorithms which I learn from the lessons: Gaussian Naive Bayes, Support Vector Classifier, K-Nearest Centroid, Random Forest Classifier and Adaboost Classifier. I decided to select 2 of them and develop further. I modified test_classifier function since I already divided features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', GaussianNB())])\n",
      "\tAccuracy: 0.82600\tPrecision: 0.36867\tRecall: 0.30600\tF1: 0.33443\tF2: 0.31677\n",
      "\tTotal predictions: 14000\tTrue positives:  612\tFalse positives: 1048\tFalse negatives: 1388\tTrue negatives: 10952\n",
      "\n",
      "Got a divide by zero when trying out: Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "\tAccuracy: 0.77464\tPrecision: 0.29174\tRecall: 0.40450\tF1: 0.33899\tF2: 0.37548\n",
      "\tTotal predictions: 14000\tTrue positives:  809\tFalse positives: 1964\tFalse negatives: 1191\tTrue negatives: 10036\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='g...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.84643\tPrecision: 0.37583\tRecall: 0.11350\tF1: 0.17435\tF2: 0.13192\n",
      "\tTotal predictions: 14000\tTrue positives:  227\tFalse positives:  377\tFalse negatives: 1773\tTrue negatives: 11623\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))])\n",
      "\tAccuracy: 0.82279\tPrecision: 0.32534\tRecall: 0.22400\tF1: 0.26532\tF2: 0.23888\n",
      "\tTotal predictions: 14000\tTrue positives:  448\tFalse positives:  929\tFalse negatives: 1552\tTrue negatives: 11071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from tester import *\n",
    "\n",
    "gau = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', GaussianNB())])\n",
    "sv = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', SVC())])\n",
    "nv = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', NearestCentroid())])\n",
    "rfc = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', RandomForestClassifier())])\n",
    "abc = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', AdaBoostClassifier())])\n",
    "\n",
    "test_classifier(gau, my_dataset, features_list)\n",
    "test_classifier(sv, my_dataset, features_list)\n",
    "test_classifier(nv, my_dataset, features_list)\n",
    "test_classifier(rfc, my_dataset, features_list)\n",
    "test_classifier(abc, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Select Algorithm\n",
    "---\n",
    "    Performances of classifiers were highly disappointing, but I have to admit that there are too few observations. Based on the performance observed, I chose Gaussian Naive Bayse and K-nearest centroids algorithms. Now, I'll try to tune parameters to see if the performance gets better. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Hyperparameters Tuning\n",
    "---\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]\n",
    "\n",
    "    In fact, choosing which alogorithm to use before tuning hyperparameters of algorithms is impoerfect because the performance of classifiers differ greatly depending on the hyperparameters. Although this fact make my logic vulnerable, I had to focus on few classifiers to tune hyperparameters because there are too many hyperparameters to tune. Tuning the hyperparameters can make best classifer for a given dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gaussian Naive Bayes\n",
    "---\n",
    "    Gaussian Naive Bayes had only one parameter: Prior. Prior means prior probabilities of the classes. However, since features were reduced through PCA, it is hard to set prior. As a result, there can be only one candidate for Gaussian NB.\n",
    "    \n",
    "    * GaussianNB()\n",
    "\tAccuracy: 0.82600\tPrecision: 0.36867\tRecall: 0.30600\tF1: 0.33443\tF2: 0.31677\n",
    "\tTotal predictions: 14000\tTrue positives:  612\tFalse positives: 1048\tFalse negatives: 1388\tTrue negatives: 10952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) K-Nearest Centroids\n",
    "---\n",
    "    K-Nearest Centroids algorithm is one of the unsupervised classifications, Clustering. There are two parameters for Neareset Centroid: metric and shrink_threshold. I can either change Manhattan or Euclidean metrics to calculate the distance. Shrink_threshold represent threshold for shrinking centroids to remove features. I looped through the possible parameters to see which one is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean:\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.0))])\n",
      "\tAccuracy: 0.77464\tPrecision: 0.29174\tRecall: 0.40450\tF1: 0.33899\tF2: 0.37548\n",
      "\tTotal predictions: 14000\tTrue positives:  809\tFalse positives: 1964\tFalse negatives: 1191\tTrue negatives: 10036\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.1))])\n",
      "\tAccuracy: 0.77321\tPrecision: 0.29393\tRecall: 0.41900\tF1: 0.34550\tF2: 0.38614\n",
      "\tTotal predictions: 14000\tTrue positives:  838\tFalse positives: 2013\tFalse negatives: 1162\tTrue negatives: 9987\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.2))])\n",
      "\tAccuracy: 0.77043\tPrecision: 0.29127\tRecall: 0.42350\tF1: 0.34515\tF2: 0.38825\n",
      "\tTotal predictions: 14000\tTrue positives:  847\tFalse positives: 2061\tFalse negatives: 1153\tTrue negatives: 9939\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.3))])\n",
      "\tAccuracy: 0.76950\tPrecision: 0.29225\tRecall: 0.43150\tF1: 0.34848\tF2: 0.39396\n",
      "\tTotal predictions: 14000\tTrue positives:  863\tFalse positives: 2090\tFalse negatives: 1137\tTrue negatives: 9910\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.4))])\n",
      "\tAccuracy: 0.76864\tPrecision: 0.29288\tRecall: 0.43800\tF1: 0.35103\tF2: 0.39851\n",
      "\tTotal predictions: 14000\tTrue positives:  876\tFalse positives: 2115\tFalse negatives: 1124\tTrue negatives: 9885\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.5))])\n",
      "\tAccuracy: 0.77000\tPrecision: 0.29947\tRecall: 0.45550\tF1: 0.36136\tF2: 0.41252\n",
      "\tTotal predictions: 14000\tTrue positives:  911\tFalse positives: 2131\tFalse negatives: 1089\tTrue negatives: 9869\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.6))])\n",
      "\tAccuracy: 0.76993\tPrecision: 0.30224\tRecall: 0.46650\tF1: 0.36682\tF2: 0.42076\n",
      "\tTotal predictions: 14000\tTrue positives:  933\tFalse positives: 2154\tFalse negatives: 1067\tTrue negatives: 9846\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.7))])\n",
      "\tAccuracy: 0.76807\tPrecision: 0.29919\tRecall: 0.46450\tF1: 0.36396\tF2: 0.41828\n",
      "\tTotal predictions: 14000\tTrue positives:  929\tFalse positives: 2176\tFalse negatives: 1071\tTrue negatives: 9824\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.8))])\n",
      "\tAccuracy: 0.76536\tPrecision: 0.29725\tRecall: 0.47100\tF1: 0.36448\tF2: 0.42170\n",
      "\tTotal predictions: 14000\tTrue positives:  942\tFalse positives: 2227\tFalse negatives: 1058\tTrue negatives: 9773\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=0.9))])\n",
      "\tAccuracy: 0.76007\tPrecision: 0.29137\tRecall: 0.47450\tF1: 0.36104\tF2: 0.42152\n",
      "\tTotal predictions: 14000\tTrue positives:  949\tFalse positives: 2308\tFalse negatives: 1051\tTrue negatives: 9692\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.0))])\n",
      "\tAccuracy: 0.75514\tPrecision: 0.28610\tRecall: 0.47750\tF1: 0.35781\tF2: 0.42115\n",
      "\tTotal predictions: 14000\tTrue positives:  955\tFalse positives: 2383\tFalse negatives: 1045\tTrue negatives: 9617\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.1))])\n",
      "\tAccuracy: 0.75014\tPrecision: 0.28252\tRecall: 0.48650\tF1: 0.35746\tF2: 0.42511\n",
      "\tTotal predictions: 14000\tTrue positives:  973\tFalse positives: 2471\tFalse negatives: 1027\tTrue negatives: 9529\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.2))])\n",
      "\tAccuracy: 0.74793\tPrecision: 0.28711\tRecall: 0.51550\tF1: 0.36881\tF2: 0.44474\n",
      "\tTotal predictions: 14000\tTrue positives: 1031\tFalse positives: 2560\tFalse negatives:  969\tTrue negatives: 9440\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.3))])\n",
      "\tAccuracy: 0.74857\tPrecision: 0.29303\tRecall: 0.53800\tF1: 0.37941\tF2: 0.46093\n",
      "\tTotal predictions: 14000\tTrue positives: 1076\tFalse positives: 2596\tFalse negatives:  924\tTrue negatives: 9404\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.4))])\n",
      "\tAccuracy: 0.74729\tPrecision: 0.29361\tRecall: 0.54700\tF1: 0.38212\tF2: 0.46648\n",
      "\tTotal predictions: 14000\tTrue positives: 1094\tFalse positives: 2632\tFalse negatives:  906\tTrue negatives: 9368\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.5))])\n",
      "\tAccuracy: 0.74507\tPrecision: 0.29404\tRecall: 0.56000\tF1: 0.38561\tF2: 0.47421\n",
      "\tTotal predictions: 14000\tTrue positives: 1120\tFalse positives: 2689\tFalse negatives:  880\tTrue negatives: 9311\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.6))])\n",
      "\tAccuracy: 0.74371\tPrecision: 0.29182\tRecall: 0.55650\tF1: 0.38287\tF2: 0.47105\n",
      "\tTotal predictions: 14000\tTrue positives: 1113\tFalse positives: 2701\tFalse negatives:  887\tTrue negatives: 9299\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.7))])\n",
      "\tAccuracy: 0.74593\tPrecision: 0.29301\tRecall: 0.55100\tF1: 0.38257\tF2: 0.46850\n",
      "\tTotal predictions: 14000\tTrue positives: 1102\tFalse positives: 2659\tFalse negatives:  898\tTrue negatives: 9341\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.8))])\n",
      "\tAccuracy: 0.74579\tPrecision: 0.29492\tRecall: 0.56050\tF1: 0.38649\tF2: 0.47496\n",
      "\tTotal predictions: 14000\tTrue positives: 1121\tFalse positives: 2680\tFalse negatives:  879\tTrue negatives: 9320\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=1.9))])\n",
      "\tAccuracy: 0.74379\tPrecision: 0.29586\tRecall: 0.57500\tF1: 0.39069\tF2: 0.48372\n",
      "\tTotal predictions: 14000\tTrue positives: 1150\tFalse positives: 2737\tFalse negatives:  850\tTrue negatives: 9263\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.0))])\n",
      "\tAccuracy: 0.74193\tPrecision: 0.29993\tRecall: 0.60450\tF1: 0.40093\tF2: 0.50245\n",
      "\tTotal predictions: 14000\tTrue positives: 1209\tFalse positives: 2822\tFalse negatives:  791\tTrue negatives: 9178\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.1))])\n",
      "\tAccuracy: 0.73893\tPrecision: 0.29812\tRecall: 0.61100\tF1: 0.40072\tF2: 0.50500\n",
      "\tTotal predictions: 14000\tTrue positives: 1222\tFalse positives: 2877\tFalse negatives:  778\tTrue negatives: 9123\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.2))])\n",
      "\tAccuracy: 0.73579\tPrecision: 0.28672\tRecall: 0.57100\tF1: 0.38175\tF2: 0.47651\n",
      "\tTotal predictions: 14000\tTrue positives: 1142\tFalse positives: 2841\tFalse negatives:  858\tTrue negatives: 9159\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.3))])\n",
      "\tAccuracy: 0.73507\tPrecision: 0.27292\tRecall: 0.51350\tF1: 0.35641\tF2: 0.43654\n",
      "\tTotal predictions: 14000\tTrue positives: 1027\tFalse positives: 2736\tFalse negatives:  973\tTrue negatives: 9264\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.4))])\n",
      "\tAccuracy: 0.74771\tPrecision: 0.24885\tRecall: 0.37950\tF1: 0.30059\tF2: 0.34344\n",
      "\tTotal predictions: 14000\tTrue positives:  759\tFalse positives: 2291\tFalse negatives: 1241\tTrue negatives: 9709\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.5))])\n",
      "\tAccuracy: 0.77229\tPrecision: 0.21579\tRecall: 0.22550\tF1: 0.22054\tF2: 0.22349\n",
      "\tTotal predictions: 14000\tTrue positives:  451\tFalse positives: 1639\tFalse negatives: 1549\tTrue negatives: 10361\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.6))])\n",
      "\tAccuracy: 0.81979\tPrecision: 0.21232\tRecall: 0.09650\tF1: 0.13269\tF2: 0.10832\n",
      "\tTotal predictions: 14000\tTrue positives:  193\tFalse positives:  716\tFalse negatives: 1807\tTrue negatives: 11284\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.7))])\n",
      "\tAccuracy: 0.83814\tPrecision: 0.23922\tRecall: 0.06100\tF1: 0.09721\tF2: 0.07168\n",
      "\tTotal predictions: 14000\tTrue positives:  122\tFalse positives:  388\tFalse negatives: 1878\tTrue negatives: 11612\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.8))])\n",
      "\tAccuracy: 0.84536\tPrecision: 0.23810\tRecall: 0.03750\tF1: 0.06479\tF2: 0.04510\n",
      "\tTotal predictions: 14000\tTrue positives:   75\tFalse positives:  240\tFalse negatives: 1925\tTrue negatives: 11760\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=2.9))])\n",
      "\tAccuracy: 0.85036\tPrecision: 0.20859\tRecall: 0.01700\tF1: 0.03144\tF2: 0.02083\n",
      "\tTotal predictions: 14000\tTrue positives:   34\tFalse positives:  129\tFalse negatives: 1966\tTrue negatives: 11871\n",
      "\n",
      "Manhattan\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.0))])\n",
      "\tAccuracy: 0.67907\tPrecision: 0.26343\tRecall: 0.69400\tF1: 0.38190\tF2: 0.52302\n",
      "\tTotal predictions: 14000\tTrue positives: 1388\tFalse positives: 3881\tFalse negatives:  612\tTrue negatives: 8119\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.1))])\n",
      "\tAccuracy: 0.67100\tPrecision: 0.25924\tRecall: 0.70150\tF1: 0.37858\tF2: 0.52304\n",
      "\tTotal predictions: 14000\tTrue positives: 1403\tFalse positives: 4009\tFalse negatives:  597\tTrue negatives: 7991\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.2))])\n",
      "\tAccuracy: 0.66450\tPrecision: 0.25672\tRecall: 0.71150\tF1: 0.37730\tF2: 0.52536\n",
      "\tTotal predictions: 14000\tTrue positives: 1423\tFalse positives: 4120\tFalse negatives:  577\tTrue negatives: 7880\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.3))])\n",
      "\tAccuracy: 0.65879\tPrecision: 0.25455\tRecall: 0.72000\tF1: 0.37613\tF2: 0.52720\n",
      "\tTotal predictions: 14000\tTrue positives: 1440\tFalse positives: 4217\tFalse negatives:  560\tTrue negatives: 7783\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.4))])\n",
      "\tAccuracy: 0.65293\tPrecision: 0.25221\tRecall: 0.72750\tF1: 0.37457\tF2: 0.52836\n",
      "\tTotal predictions: 14000\tTrue positives: 1455\tFalse positives: 4314\tFalse negatives:  545\tTrue negatives: 7686\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.5))])\n",
      "\tAccuracy: 0.65057\tPrecision: 0.25172\tRecall: 0.73300\tF1: 0.37474\tF2: 0.53024\n",
      "\tTotal predictions: 14000\tTrue positives: 1466\tFalse positives: 4358\tFalse negatives:  534\tTrue negatives: 7642\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.6))])\n",
      "\tAccuracy: 0.65021\tPrecision: 0.25277\tRecall: 0.74050\tF1: 0.37689\tF2: 0.53431\n",
      "\tTotal predictions: 14000\tTrue positives: 1481\tFalse positives: 4378\tFalse negatives:  519\tTrue negatives: 7622\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.7))])\n",
      "\tAccuracy: 0.65400\tPrecision: 0.25726\tRecall: 0.75350\tF1: 0.38356\tF2: 0.54373\n",
      "\tTotal predictions: 14000\tTrue positives: 1507\tFalse positives: 4351\tFalse negatives:  493\tTrue negatives: 7649\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.8))])\n",
      "\tAccuracy: 0.65757\tPrecision: 0.26160\tRecall: 0.76650\tF1: 0.39008\tF2: 0.55303\n",
      "\tTotal predictions: 14000\tTrue positives: 1533\tFalse positives: 4327\tFalse negatives:  467\tTrue negatives: 7673\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=0.9))])\n",
      "\tAccuracy: 0.66364\tPrecision: 0.26464\tRecall: 0.76150\tF1: 0.39278\tF2: 0.55362\n",
      "\tTotal predictions: 14000\tTrue positives: 1523\tFalse positives: 4232\tFalse negatives:  477\tTrue negatives: 7768\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.0))])\n",
      "\tAccuracy: 0.66929\tPrecision: 0.26676\tRecall: 0.75200\tF1: 0.39382\tF2: 0.55140\n",
      "\tTotal predictions: 14000\tTrue positives: 1504\tFalse positives: 4134\tFalse negatives:  496\tTrue negatives: 7866\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.1))])\n",
      "\tAccuracy: 0.67336\tPrecision: 0.26639\tRecall: 0.73350\tF1: 0.39084\tF2: 0.54305\n",
      "\tTotal predictions: 14000\tTrue positives: 1467\tFalse positives: 4040\tFalse negatives:  533\tTrue negatives: 7960\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.2))])\n",
      "\tAccuracy: 0.67900\tPrecision: 0.26899\tRecall: 0.72600\tF1: 0.39254\tF2: 0.54187\n",
      "\tTotal predictions: 14000\tTrue positives: 1452\tFalse positives: 3946\tFalse negatives:  548\tTrue negatives: 8054\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.3))])\n",
      "\tAccuracy: 0.68771\tPrecision: 0.26337\tRecall: 0.66000\tF1: 0.37650\tF2: 0.50722\n",
      "\tTotal predictions: 14000\tTrue positives: 1320\tFalse positives: 3692\tFalse negatives:  680\tTrue negatives: 8308\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.4))])\n",
      "\tAccuracy: 0.71879\tPrecision: 0.25186\tRecall: 0.49150\tF1: 0.33305\tF2: 0.41292\n",
      "\tTotal predictions: 14000\tTrue positives:  983\tFalse positives: 2920\tFalse negatives: 1017\tTrue negatives: 9080\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.5))])\n",
      "\tAccuracy: 0.79086\tPrecision: 0.22802\tRecall: 0.19450\tF1: 0.20993\tF2: 0.20039\n",
      "\tTotal predictions: 14000\tTrue positives:  389\tFalse positives: 1317\tFalse negatives: 1611\tTrue negatives: 10683\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.6))])\n",
      "\tAccuracy: 0.85114\tPrecision: 0.16129\tRecall: 0.01000\tF1: 0.01883\tF2: 0.01231\n",
      "\tTotal predictions: 14000\tTrue positives:   20\tFalse positives:  104\tFalse negatives: 1980\tTrue negatives: 11896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "print \"Euclidean:\"\n",
    "for i in range(30):\n",
    "    nv = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', NearestCentroid(shrink_threshold = i/float(10)))])\n",
    "    test_classifier(nv, my_dataset, features_list)\n",
    "print \"Manhattan\"\n",
    "for i in range(17):\n",
    "    nv = Pipeline([('imputer', Imputer(missing_values='NaN', strategy='mean', axis=0)), ('scale', StandardScaler()), ('pca', PCA(n_components = 8)), ('clf', NearestCentroid(metric = \"manhattan\", shrink_threshold = i/float(10)))])\n",
    "    test_classifier(nv, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * NearestCentroid(metric='euclidean', shrink_threshold=0.6))])\n",
    "\tAccuracy: 0.76993\tPrecision: 0.30224\tRecall: 0.46650\tF1: 0.36682\tF2: 0.42076\n",
    "\tTotal predictions: 14000\tTrue positives:  933\tFalse positives: 2154\tFalse negatives: 1067\tTrue negatives: 9846\n",
    "\n",
    "\n",
    "    The best classifier of Nearest Centroids was the one with Euclidean metric and shrinks_threshold of 0.6. It had accuracy of 0.77, precision of 0.30, recall of 0.46 and f1 score of 0.36.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Check the effects of  new features\n",
    "---\n",
    "    To validate the affect of new features I made, I compared with features list without new features('def_income_r', 'es_r', 'from_r', 'to_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', GaussianNB())])\n",
      "\tAccuracy: 0.82821\tPrecision: 0.35928\tRecall: 0.25850\tF1: 0.30067\tF2: 0.27386\n",
      "\tTotal predictions: 14000\tTrue positives:  517\tFalse positives:  922\tFalse negatives: 1483\tTrue negatives: 11078\n",
      "\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=8, whiten=False)), ('clf', NearestCentroid(metric='manhattan', shrink_threshold=1.6))])\n",
      "\tAccuracy: 0.83243\tPrecision: 0.22627\tRecall: 0.07150\tF1: 0.10866\tF2: 0.08283\n",
      "\tTotal predictions: 14000\tTrue positives:  143\tFalse positives:  489\tFalse negatives: 1857\tTrue negatives: 11511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list2 = ['poi','salary', 'bonus', 'long_term_incentive', 'deferral_payments', 'loan_advances', 'other', 'total_stock_value',\\\n",
    "                'shared_receipt_with_poi']\n",
    "\n",
    "test_classifier(gau, my_dataset, features_list2)\n",
    "test_classifier(nv, my_dataset, features_list2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Gaussian\n",
    "    with:    Accuracy: 0.82600    Precision: 0.36867    Recall: 0.30600    F1: 0.33443    F2: 0.31677\n",
    "    without: Accuracy: 0.82821\t Precision: 0.35928\t   Recall: 0.25850\t  F1: 0.30067\t F2: 0.27386\n",
    "\n",
    "    * Nearest Centroid\n",
    "    with:    Accuracy: 0.76993    Precision: 0.30224    Recall: 0.46650    F1: 0.36682    F2: 0.42076\n",
    "    without: Accuracy: 0.76143\t Precision: 0.26976\t   Recall: 0.39250\t  F1: 0.31976\t F2: 0.35976\n",
    "    \n",
    "    Comparing the result of two, both classifier with the new variables shows high performance. Gaussian without new variables showed low recall and nearest centroid showed low precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation\n",
    "---\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "    If I use all the data for training, it would be easier to form perfect classifier. However, the classifer would be overfit to the dataset I train and would perform worse when new data are introduced. To prevent this, it is important to divide dataset into training and test set. Use training set to train the classifer and use test set to check the validation for new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Validation method\n",
    "---\n",
    "    In tester.py file, test_classifier use stratified shuffle split to validate the classifiers. Stratified shuffle split returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. This method is appropriate for our dataset for three reasons. First, this reduce the risk of overfiting by making several shuffle, making stratified shuffle split better than jest spliting normal. Second, stratified shuffle split is appropriate for our data because of the size of dataset. Since we have small number of observation compared to the number of features, it is important to validate a number of times to avoid extreme figures. Lastly, shuffling is important when the numbers of observation among labels differ greatly. Since the number of POI is small, it is easy to result several outcomes with high variance. So it is effective to use stratified shuffle split to average the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Comparison\n",
    "---\n",
    "\n",
    "    * GaussianNB()\n",
    "\tAccuracy: 0.82600\tPrecision: 0.36867\tRecall: 0.30600\tF1: 0.33443\tF2: 0.31677\n",
    "\tTotal predictions: 14000\tTrue positives:  612\tFalse positives: 1048\tFalse negatives: 1388\tTrue negatives: 10952\n",
    "\n",
    "    * NearestCentroid(metric='euclidean', shrink_threshold=0.6))])\n",
    "\tAccuracy: 0.76993\tPrecision: 0.30224\tRecall: 0.46650\tF1: 0.36682\tF2: 0.42076\n",
    "\tTotal predictions: 14000\tTrue positives:  933\tFalse positives: 2154\tFalse negatives: 1067\tTrue negatives: 9846\n",
    "\n",
    "    To compare the both classifier, both had precision and recall over 0.3. However, Gaussian NB has strength in accuracy and precision, while nearest centroid has strength in recall. Although F1 score of Nearest Centroid, it is hard to tell which one is definately better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Interpretation\n",
    "---\n",
    "    In this special dataset, accuracy is not that important since the proportion of two labels is so imbalanced. Therefore, precision, recall or mixture of those, f1 score is important for this dataset. Gaussian Naive Bayes classifier had relativly high accuracy and precision. Especially, precision is 0.36 meaning that 36% of positively designated observations are true positives. This is meaningful considering there are only 13% POI in the whole dataset, and the classifier meningfully reduce the scope. On the other hand, Neareset centroid have relatively high level of recall which is 0.46, meaning that 46% of real POI are classified positive. These two values have trade off, since the more classifier classify observation positive, the higher the recall but the lower the precision and vice versa. f1-score which is mixture of those two seems fair to compare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Dump classifiers\n",
    "---\n",
    "    I also modified the dump classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(gau, my_dataset, features_list)\n",
    "dump_classifier_and_data(nv, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "---\n",
    "    Although I found two classifiers with above 0.3 precision and recall, the overall performance in not quite satisfactory. There can be some limitation regarding this research.\n",
    "    1) Dealing of Nan values. Since there were too many missing values on the data, I had to put the average values on the all of Nan space. However, there can be other ways to deal with the Nan values, such as deleting observation with too much missing values.\n",
    "    2) Scaling: I scaled all the features with their means and variances because it would be fair for all the features with different scales. However, Scaling lead very different result compared to the non-scaled result. Scaling may affect the actual information of data.\n",
    "    3) Ensemble methods: Usually ensemble methods shows good performances in many data. I didn't try to tune the parameter of ensemble methods since they showed poor performance in the first place. However, there is possibility that ensemble method lead better performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
